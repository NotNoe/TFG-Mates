\chapter{Método de las diferencias finitas}
\label{cap:dif_fin}

Como adelantamos en el Capítulo \ref{cap:introduccion}, los métodos de las diferencias finitas aproximan las soluciones de una ecuación diferencial cualquiera sustituyendo las derivadas parciales por un cociente evaluando la función en puntos próximos.

\section{Diferencias finitas}
Existen varios coeficientes incrementales por los que se pueden aproximar las derivadas parciales. En esta sección definiremos los que vamos a utilizar. Técnicamente, necesitaríamos definir el cociente incremental para cada una de las variables por separado y tanto para las funciones de dos como tres variables, no obstante, la definición es análoga en todos estos casos, por lo que en esta sección daremos la definición sobre la primera variable de una función de dos variables.

Por tanto, supondremos que tenemos una función $f\in D$, siendo $D$ un abierto de $\mathbb{R}^2$, y queremos aproximar su derivada parcial en un punto $x\in D$ por el valor de la función en puntos cercanos. Más adelante haremos más hincapié en qué puntos en concreto utilizaremos. Por ahora, simplemente supongamos que tenemos un número real $\Delta x>0$ de manera que $x\pm \Delta x \in D$.

\subsection{Cociente incremental progresivo}
La definición del cociente incremental progresivo sobre la primera variable es
\begin{equation}
	\label{eq:not_ford}
	f^{x}(x,t) := \frac{f(x+\Delta x,t)-f(x,t)}{\Delta x}.
\end{equation}
Además, si $\frac{\partial f}{\partial x}$ existe, podemos obtener el siguiente resultado:
\begin{lema} \label{lema:error_prog}
	Si existe $\frac{\partial f}{\partial x}$, si definimos el error al aproximar la derivada por el cociente incremental progresivo como $h(\Delta x)$ se tiene que tiende a 0 cuando $Deltax$ tiende a 0. Más concretamente, sea
	\begin{equation}
		h(\Delta x) := \left| f^{x}(x,t) - \frac{\partial f}{\partial x}(x,t) \right|,
	\end{equation}
	se tiene que
	\begin{equation}\label{eq:lemachap2_1}
		h(\Delta x) \xrightarrow[\Delta x\longrightarrow 0]{} 0.
	\end{equation}
\end{lema}

\begin{proof}
	Sea $t$ fijo, si consideramos la función $f$ únicamente como una función sobre la variable $x$, podemos hacer su desarrollo de Taylor de primer orden centrado en el punto $x$, lo que nos resulta en
	\begin{equation}
		f(x+\Delta x,t) = f(x,t) + \Delta x\frac{\partial f}{\partial x}(x,t) + h_1(x+\Delta x)\Delta x, 
	\end{equation}
	que despejando nos lleva a
	\begin{equation}
		\frac{f(x+\Delta x,t) - f(x,t)}{\Delta x} - \frac{\partial f}{\partial x}(x,t) = h_1(x+\Delta x) \Rightarrow f^{x}(x,t) - \frac{\partial f}{\partial x}(x,t) = h_1(x+\Delta x).
	\end{equation}
	Por tanto, el error que cometemos al hacer la aproximación es $h(\Delta x) = |h_1(x+\Delta x)|$. Pero como el resto del desarrollo de Taylor siempre tiende a 0 cuando nos aproximamos a $x$, es claro que se cumple \eqref{eq:lemachap2_1}.
\end{proof}

\section{Cociente incremental regresivo}
De manera muy parecida al progresivo, definimos el regresivo como
\begin{equation}
	\label{eq:not_back}
	f^{\bar{x}}(x,t) := \frac{f(x,t)-f(x-\Delta x,t)}{\Delta x}.
\end{equation}
y, si la función $f$ es diferenciable respecto a la variable $x$, tenemos el siguiente resultado:

\begin{lema}\label{lema:error_reg}
	Si existe $\frac{\partial f}{\partial x}$, si definimos el error al aproximar la derivada por el cociente incremental regresivo como $h(\Delta x)$ se tiene que tiende a 0 cuando $Delta x$ tiende a 0. Más concretamente, sea
	\begin{equation}
		h(\Delta x) := \left| f^{\bar{x}}(x,t) - \frac{\partial f}{\partial x}(x,t) \right| ,
	\end{equation}
	se tiene que
	\begin{equation}\label{eq:lemachap2_2}
		h(x+\Delta x) \xrightarrow[\Delta x\longrightarrow 0]{} 0.
	\end{equation}
\end{lema}

\begin{proof}
	Si hacemos el mismo desarrollo de Taylor que en la demostración del lema anterior, pero lo evaluamos en el punto $x-\Delta x$ en lugar de $x+\Delta x$, la demostración es completamente análoga.
\end{proof}

\com{¿Es suficientemente análoga y sencilla como para omitirla, o debería de volver a hacer la demostración completa?}

\subsection{Segundo cociente incremental}
Necesitaremos también aproximar las derivadas parciales de segundo orden. Esto lo haremos mediante el siguiente cociente incremental, que no es más que el resultado de aplicar los dos cocientes incrementales anteriores de manera secuencial:
\begin{equation}
	\label{eq:not_second}
	f^{x\bar{x}}(x,t) := \frac{f(x+\Delta x,t) - 2f(x,t) + f(x-\Delta x)}{\Delta x^2}.
\end{equation}
Aunque la demostración es ligeramente más compleja, podemos obtener un resultado equivalente respecto al error de la aproximación.

\begin{lema} \label{lema:error_segunda}
	Si existe $f$ es tres veces derivable respecto a la variable $x$, si definimos el error al aproximar la derivada por el segundo cociente incremental como $h(\Delta x)$ se tiene que tiende a 0 cuando $Delta x$ tiende a 0. Más concretamente, sea
	\begin{equation}
		h(\Delta x) := \left| f^{x\bar{x}}(x,t) - \frac{\partial^2 f}{\partial x^2}(x,t) \right| ,
	\end{equation}
	se tiene que
	\begin{equation}\label{eq:lemachap2_3}
		h(x+\Delta x) \xrightarrow[\Delta x\longrightarrow 0]{} 0.
	\end{equation} 
\end{lema}

\begin{proof}
	Si hacemos el desarrollo de Taylor de orden 3 centrado sobre $x$ para los puntos $x+\Delta x$ y $x-\Delta x$ obtenemos
	\begin{equation}
		f(x+\Delta x,t) = f(x,t) + \Delta x\frac{\partial f}{\partial x} + \Delta x^2 \frac{1}{2} \frac{\partial^2 f}{\partial x^2} + \Delta x^3 \frac{1}{6} \frac{\partial^3 f}{\partial x^3} + \Delta x^3 h_3(x+\Delta x)
	\end{equation}
	y
	\begin{equation}
		f(x-\Delta x,t) = f(x,t) - \Delta x\frac{\partial f}{\partial x} + \Delta x^2 \frac{1}{2} \frac{\partial^2 f}{\partial x^2} - \Delta x^3 \frac{1}{6} \frac{\partial^3 f}{\partial x^3} + \Delta x^3 h'_3(x+\Delta x)
	\end{equation}
	respectivamente. Si sumamos ambas ecuaciones, obtenemos
	\begin{equation}
	f(x+\Delta x,t) + f(x-\Delta x,t) = 2f(x,t) + \Delta x^2 \frac{\partial^2 f}{\partial x^2} + \Delta x^3(h_3(x+\Delta x) + h'_3(x-\Delta x)),
	\end{equation}
	que al despejar y aplicar la definición de segundo cociente incremental, se nos queda en
	\begin{equation}
		f^{x\bar{x}} - \frac{\partial^2 f}{\partial x^2} = \Delta x (h_3(x+\Delta x)+h'_3(x-\Delta x)).
	\end{equation}
	Por tanto, el error es $h(\Delta x) = \Delta x (h_3(x+\Delta x)+h'_3(x-\Delta x))$, que claramente cumple \eqref{eq:lemachap2_3}.
\end{proof}

\section{Mallas}\label{sec:malla}
Cuando definamos y trabajemos sobre algoritmos numéricos para aproximar las soluciones de los problemas necesitaremos discretizar el dominio, pues necesitamos trabajar con una cantidad de puntos finita para que un ordenador pueda implementar el algoritmo.

Para conseguir esto, definiremos mallas, que son subconjuntos (potencialmente infinitos pero numerables) del dominio (que denotaremos como $R$) del problema en cuestión. Luego aproximaremos la solución exacta del problema en una cantidad finita de puntos de la malla.

\begin{definicion}[Malla bidimensional]\label{def:malla2d}
	Dado una ecuación diferencial con dominio $R\subseteq\mathbb{R}^2$, sean $a$, $b$, $c$ y $d$ números reales tales que $a\leq b$ y $c\leq d$, y sean $n_1$ y $n_2$ dos números naturales mayores que 1, definimos la malla bidimensional (o simplemente malla) como
	\begin{equation}
		M^2(a,b,c,d,n_1,n_2) := \{(a+i\Delta_1,c+j\Delta_2) \hspace{5px} | \hspace{5px} i,j\in\mathbb{Z}\}\cap R,
	\end{equation}
	siendo $\Delta_1:=\frac{b-a}{n_1-1}$ y $\Delta_2:=\frac{d-c}{n_2-1}$.
\end{definicion}
\begin{definicion}[Malla tridimensional]\label{def:malla3d}
	Dado una ecuación diferencial con dominio $R\subseteq\mathbb{R}^3$, sean $a$, $b$, $c$, $d$, $e$ y $f$ números reales tales que $a\leq b$ y $c\leq d$, y sean $n_1$, $n_2$ y $n_3$ tres números naturales mayores que 1, definimos la malla tridimensional (o simplemente malla) como
	\begin{equation}
		M^3(a,b,c,d,e,f,n_1,n_2,n_3) := \{(a+i\Delta_1,c+j\Delta_2,e+k\Delta_3) \hspace{5px} | \hspace{5px} i,j,k\in\mathbb{Z}\}\cap R,
	\end{equation}
	siendo $\Delta_1:=\frac{b-a}{n_1-1}$, $\Delta_2:=\frac{d-c}{n_2-1}$ y $\Delta_3:=\frac{f-e}{n_3-1}$.
\end{definicion}

Normalmente los parámetros de la construcción de la malla están fijos en cada problema, por lo que, si no hay lugar a confusión, nos referiremos a ella simplemente como $M$, omitiendo los parámetros y la dimensión.

Además, cada una de las dimensiones de la malla van a corresponder siempre a $x$, $y$ o $t$, por lo que si la dimensión $i$ del problema es $x$, diremos $\Delta x$ y $n_x$ en lugar de $\Delta_i$ y $n_i$ para que la notación sea más intuitiva.

Supongamos que tenemos un problema con un dominio $R$ y una malla $M$ tridimensional, sin pérdida de la generalidad podemos decir que $x$ está asociado a la primera dimensión, $y$ a la segunda y $t$ a la tercera. Denotaremos pues $x_i:=a+i\Delta x$, $y_j:=c+j\Delta y$ y $t_k:=e+k\Delta t$. Es importante apreciar que con esta notación, los puntos de la malla serán todos de la forma $(x_i,y_j,t_k)$\footnote{Nótese que todos los puntos de la malla serán de esa forma para alguna terna $(i,j,k)\in\mathbb{Z}^3$, pero el recíproco no es necesariamente cierto. Dependiendo del dominio $R$, puede pasar que haya ternas $(i,j,k)\in\mathbb{Z}^3$ tales que $(x_i,y_j,t_k)\notin R$ y por tanto el punto no pertenezca a la malla.} y que se cumple la relación $x_{n_x-1}=a+(n_x-1)*\Delta x = a + (n_x-1)*\frac{b-a}{n_x-1}=b$ para cualquiera de las tres dimensiones (cambiando $x$ por $y$ o $t$ y $a,b$ por $c,d$ o $e,f$).

Una última observación necesaria es que, si el dominio $R$ es convexo (lo cuál se cumplirá en todos los problemas en los que trabajamos), todos los puntos $(x_i,y_j,t_k)$ tales que $0\leq i<n_x$, $0\leq j<n_y$ y $0\leq k<n_t$ estarán dentro de la malla.

\subsubsection{Significado de las mallas}
La idea intuitiva tras la construcción de las mallas es que el objetivo de los algoritmos será aproximar el valor de las funciones sobre una región del espacio acotada durante un tiempo concreto $T$. Como necesitamos trabajar con una cantidad finita de puntos, hacemos mallas con una densidad concreta (que afectará al rendimiento y la precisión).

Los puntos que tienen todos sus índices dentro de los límites $[0,n_i)$ son los puntos que están dentro de esta superficie finita (que tiene forma de rectángulo en dos o tres dimensiones) que queremos calcular, pero puede ser que necesitemos aproximar la función en otros puntos para poder calcularlo.

\section{Notación}\label{sec:notacion}
Haremos ahora algunas definiciones sobre la notación con el objetivo de hacer las demostraciones futuras más legibles.

$\bullet$ Sea $f$ una función cualquiera definida en $R$, denotaremos $f_{i,j,k}$ el resultado de evaluar la función en los respectivos puntos de la malla, es decir $f_{i,j,k}:=f(x_i,y_j,t_k)$

$\bullet$ La solución del problema que estemos trabajando la denotaremos por $u$.

$\bullet$ La aproximación de la solución, que es una función definida solo sobre la malla $M$, la llamaremos $U$.



